The \TVB simulator resembles popular neural network simulators in 
many fundamental ways, both mathematically and in terms of informatics 
structures, however we have found it necessary to introduce auxiliary
concepts particularly useful in the modeling of large scale brain 
networks.

% need to cite Andreas' work and 

\subsection{Node dynamics}

	In \TVB, nodes are not considered to be abstract neurons nor necessarily 
	small groups thereof, but rather large populations of neurons. 

	\note[mw] {Highlight use of statistical mode models?}


	\note[mw]{Discuss the differing assumption sof mass models. Discuss how
		the goal is not to reproduce precise traces of neurons (like in the
		case of Brett's adaptive exponential model) but rather reproduce
	mass-scale phenomena (oscillation) and network phenomena. }

\subsection{Network structure}

	The network of neural masses in TVB simulations directly follows from 
	a pair of 
	geometrical constraints on cortical dynamics. The first is the 
	large-scale white matter fibers that form a non-local and hetergeneous
	(translation variant) connectivity, either measured by anatomical
	tracing (CoCoMac\ref{CoCoMac}) or diffusion-weighted imaging. The second
	is that of horizontal projections along the surface, which are modeled
	through a translation invarient connectivity kernel, approximating a 
	neural field. 

	\subsubsection{Large-scale connectivity}

	The large-scale region level connectivity at the scale of centimeters,
	resembles more a traditional
	neural network than a neural field in that neural space is discrete, 
	each node corresponding to a neuroanatomical region of interest, such
	as V1, etc. It is at this level that inter-regional delays play a large
	role. 

	It is often seen in the literature that the inter-node coupling functions
	\textit{are} part of the node model itself. In \TVB, we have instead 
	chosen to factor such models into the intrinsic neural mass dynamics, where each 
	neural mass's equations specify how connectivity contributes to the
	node dynamics, and the coupling function, which specifies how the activity
	from each region is mapped through the connectivity matrix. Common coupling 
	functions are provided such as the linear, difference and periodic functions
	often used in the literature.

	\subsubsection{Local connectivity}

	The local connectivity of the cortex at the scale of millimeters provides
	a continuous 2D surface along horizontal projections connect 
	cortical columns. Such a structure has previously been modeled by
	neural fields \cite{Amari_1977,Jirsa_1997}. In \TVB, a cortical mesh, 
	as obtained from structural MRI and simplified, provides a spatial 
	discretization on which neural masses are placed and connected with a
	connectivity kernel, itself only a function of the geodesic distance
	between the two masses, and this is considered to provide an
	adequate approximation of a neural field, depending on the properties
	of the mesh and the imaging modalities that sample the activity simulated
	on the mesh \ref{Spiegler_2013}.

	\TVB currently provides several connectivity kernels, among others, 
	the Laplacian and Gaussian kernels. Once a cortical surface mesh 
	and connectivity kernel and its parameters are chosen, the geodesic
	distance (i.e. the distance along the cortical surface) is evaluated
	between all neural masses \cite{Mitchell1987}, and a cutoff is chosen
	past which the kernel falls to 0. This results in a sparse matrix that 
	is used during integration to implement the approximate neural field. 

\subsection{Integration of stochastic delay differential equations}

	In order to obtain numerical approximations of the neural model 
	described above, \TVB provides several integrators. For
	deterministic simulations, the well-known Euler, 
	Heun and Runge-Kutta 4th order algorithms are available. For stochastic
	simulations, stochastic Euler and Heun integrators are available, 
	following recent literature on numerical solutions to stochastic
	differential equations \cite{Kloeden_1995,Mannella_2002,Mannella_1989}.

	While the literature on numerical treatment of delayed or 
	stochastic systems exists, it is less well known how to treat 
	the presence of both. For the moment, the methods implemented by \TVB
	treat stochastic integration separately from delays. 
	This separation conincides with a modeling assumption that in
	\TVB the dynamical phenomena to be studied are largely determined
	by the interaction of the network structure and neural mass dynamics, 
	and that stochastic fluctuations do not fundamentally reorganize the
	solutions of the system \ref{Ghosh_2008,Deco_2009,Deco_2011,Deco_Senden_2012}.

	Due to such a separation, the implementation of delays in the
	regional coupling is performed outside the integration step,
	by indexing a circular buffer containing the recent simulation 
	history, and providing a matrix of delayed state data to the 
	network of neural masses. While the number of pairwise
	connections rises with $N^2$, only one buffer per region 
	is stored with a length corresponding to the longest projection
	from that region.

\subsection{Forward solutions}

	A primary goal of \TVB is not only to model neural activity itself
	but just as importantly the imaging modalities common in human 
	neurosciences, using so-called forward solutions, which allow for
	the projection of neural activity into sensor space. To account
	parsimoniously for other ways in which simulated data might be saved, 
	such as simple temporal averaging, we refer to each of these simply as 
	\textit{Monitors}, which take as input neural activity and 
	output a particular projection thereof. In most cases, this 
	takes the discrete-time form of

	\[ \hat{y}[j, t] = \sum_{i=1, \tau=1}^{N_W, N_k} W[j, i] K[\tau] y[i, t-\tau] \]

	\noindent where $y[i, t]$ is the amplitude of the $i^{th}$ neural mass at time
	$t$, $K[\tau]$ is a temporal kernel, and $W[j, i]$ is a spatial kernel,
	usually projecting the state variable of interest of the $i^{th}$ 
	neural mass to the $j^{th}$ sensor. 

	Where necessary for computational reasons, monitors employ more than 
	one internal buffer. The fMRI monitor is one 
	example: given a typical sampling frequency of simulation may be upward of 
	64 kHz, and the haemodynamic response function may last several seconds, 
	requiring many gigabytes of memory for the fMRI monitor alone. Given that 
	the time-scale of simulation and fMRI differ by several orders of magnitude, 
	the subsequent averaging and downsampling is entirely justified. 

	In the cases of the EEG and MEG monitors, $K$ implements a simple
	temporal average, and $W$ consists of a so-called lead-field matrix as typically
	derived from a combination of structural imaging data of the patient 
	and the locations and orientations of the neural sources and the locations
	and orientions of the EEG electrodes and MEG gradiometers and magnetometers. 
	As the development and implementation of such lead-fields is well developed
	elsewhere \cite{Jirsa_2002,Nolte2003,Gramfort_2010}, \TVB provides access
	to the well-known OpenMEEG package, however, the user is free to provide 
	his or her own.

\subsection{Native code generation for C \& GPU}

	Several of the core components (integrators, mass models, coupling
	functions) have targeted towards a C source code backend, which has
	allowed for the compilation of simulations to native code loaded 
	either as a shared library accessed via the \texttt{ctypes} modules
	or as CUDA kernels accessed via the PyCUDA \cite{PyCUDA}.
	While such an approach may provide speed ups, they depend on the
	presence of a C compiler and, in the case of GPU, the CUDA toolkit and
	a compatible graphics card, and in the future, prepackaged versions of \TVB
	will include precompiled objects for most kinds of simulations. 

	The approach used in compiling a simulation to native code takes advantage
	of the fact that CUDA is quite similar to C, and thus a generic template
	abstracts much of the boilerplate between the two. For each part of the 
	simulator, a generic function is customized with a class specific kernel;
	for example, in the case of a neural mass model, we have in the Python class

	\begin{lstlisting}
	class Generic2dOscillator(Model):
	    tau = FloatArray(...)
	    # etc.

	    device_info = model_device_info(
		pars=[tau, a, b, c, d, I],
		kernel="""
		float tau  = P(0)
		    , a    = P(1) ; // etc

		// state variables
		    , v    = X(0)
		    , w    = X(1)

		// aux variables
		    , c_0  = I(0)   ;

		// derivatives
		DX(0) = d * 
		(tau * (w - v*v*v + 3.0*v*v + I + c_0));
		DX(1) = d * 
		((a + b*v + c*v*v - w) / tau);
		"""
	    )
	\end{lstlisting}

	\noindent where the device\_info attribute is used to specify how the
	class's mathematical description fits into the general model function:

	\begin{lstlisting}
	/* wrapper for model specific code computing RHSs of diff-eqs */
	__device__
	void model_dfun(
	  float * _dx, float *_x, float *mmpr, float *input)
	{
	#define X(i) _x[n_thr*i]
	#define DX(i) _dx[n_thr*i]
	#define P(i) mmpr[n_thr*i]
	#define I(i) input[i]

	    // begin model code
	    \$model_dfun
	    // end model specific code

	#undef X
	#undef DX
	#undef P
	#undef I
	\end{lstlisting}

	\noindent where the C preprocessor defines allow the model specific
	kernel to easily reference the correct parts of the multidimensional 
	per-thread arrays (in the case of the GPU). 

	 \begin{figure}
		{\includegraphics[width=0.48\textwidth]{images/gpu_dxdt.png}}
		\caption{
		Right A typical parameter space exploration, 32 x 32 grid of
		coupling strength (y-axis) v. neural excitability (x-axis).
		This grid of simulations was run on both TVB's Python/NumPy
		implementation and the new GPU backend for 200 ms simulation
		time with otherwise default parameters. The former took ~2
		hours and the latter ~ 1 min. Left Quantitative comparison of
		solutions and instantaneous derivatives is shown for an even
		sampling of the parameter space across k where a = -2, because
		this slice showed the most error on the GPU. 	
		}
		\label{fig:gpu_dxdt}
	\end{figure}

	 \begin{figure}
		{\includegraphics[width=0.48\textwidth]{images/gpu_pse.png}}
		\caption{}
		\label{fig:gpu_pse}
	\end{figure}

	As can be seen in the listing, the calculations
	in native code are performed with 32-bit floating point numbers, and it
	is reasonable to ask if this is numerically accurate. In Fig 
	\ref{fig:gpu_pse}, we present a parameter space exploration performed with
	both the pure Python NumPy simulator and the GPU simulator, showing the 
	isocontours of average standard deviation in the parameter space. Some
	deviation can be identified visually in parts of the parameter space, and
	in in Fig \ref{fig:gpu_dxdt}, we show in more detail time series of 
	the Python and GPU solutions.

	 \begin{figure}
		{\includegraphics[width=0.48\textwidth]{images/gpu_acceleration.png}}
		\caption{}
		\label{fig:gpu_pse}
	\end{figure}

	This approach allows significant acceleration of parameter sweeps in the
	case of the GPU by taking
	advantage of the fact that in many cases, only numerical values vary
	between different threads and not memory access patterns. Where one of the
	dimensions of a parameter sweep implies changing memory access patterns, 
	for example conduction velocity, it is advantageous to reorder the parameters,
	so that such memory varying parameters only change between grids of GPU
	threads and not within.

	In Fig \ref{fig:gpu_acceleration}, we plot the speedup brought by the GPU
	over the Python NumPy simulator as a function of the number of simulations 
	performed simulataneously on the GPU.



\subsection{Other simulators compared to \TVB}

	Brian should be a particular focus in this section, as it may
	be one of the closest. 

	- several spiking models in the literature use the notain of a reset
		condition and potential, for which Brian provides explicit
		functionality. TVB does not because typically the deterministic
		dynamics of neural mass models are continuous.
